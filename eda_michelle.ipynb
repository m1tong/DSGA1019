{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blThwcZo0Q-V"
   },
   "source": [
    "### Research questions\n",
    "1. prediction problem (classification) and feature weights - simply predict the next sentiment based on generated features, and analyse which feature contributes the most. examples: user-id (same thing) date (time), text (key words).\n",
    "2. incoperate with LLM to give explanations of why the text is classified as given sentiment.\n",
    "3. efficient forecasting over large datasets, create a basic model, and compared two ways of processing data. 1, deploy locally and use naive python packages. 2, utilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 1137,
     "status": "ok",
     "timestamp": 1744828695063,
     "user": {
      "displayName": "Michelle Tong",
      "userId": "01244029464473717573"
     },
     "user_tz": 240
    },
    "id": "gCxBRGXvwynN"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/michelletong/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GroupKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import multiprocessing\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "executionInfo": {
     "elapsed": 280,
     "status": "error",
     "timestamp": 1744828701560,
     "user": {
      "displayName": "Michelle Tong",
      "userId": "01244029464473717573"
     },
     "user_tz": 240
    },
    "id": "4XMFiO5Lw3RQ",
    "outputId": "0fd86710-1545-44ab-fb06-88c2fcbf0ed6"
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the dataset\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(file_path, encoding='latin-1', header=None)\n",
    "    df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "    \n",
    "    # Convert sentiment to binary (0: negative, 1: positive)\n",
    "    # Assuming sentiment values are 0 and 4 in the original dataset\n",
    "    df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "    \n",
    "    # Convert date to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], format='%a %b %d %H:%M:%S PDT %Y')\n",
    "    \n",
    "    # Extract basic features from text\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    df['hashtag_count'] = df['text'].str.count(r'#')\n",
    "    df['mention_count'] = df['text'].str.count(r'@')\n",
    "    df['url_count'] = df['text'].str.count(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\\\\\\\\\\\\\(\\\\\\\\\\\\\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df['hour'] = df['date'].dt.hour\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the text data by removing @mentions, URLs, hashtags, punctuation\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs - more comprehensive pattern\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove @mentions - more comprehensive pattern\n",
    "    text = re.sub(r'@[\\w_]+', '', text)\n",
    "    \n",
    "    # Remove hashtags (but keep the text after #)\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove digits (optional - uncomment if needed)\n",
    "    # text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove extra whitespace (including newlines)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_text_cleaning(text):\n",
    "    \"\"\"\n",
    "    Function to debug text cleaning process\n",
    "    \"\"\"\n",
    "    print(\"Original:\", text)\n",
    "    \n",
    "    # Test URL removal\n",
    "    text_no_urls = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    print(\"After URL removal:\", text_no_urls)\n",
    "    \n",
    "    # Test @mention removal\n",
    "    text_no_mentions = re.sub(r'@[\\w_]+', '', text_no_urls)\n",
    "    print(\"After @mention removal:\", text_no_mentions)\n",
    "    \n",
    "    # Test hashtag conversion\n",
    "    text_no_hashtags = re.sub(r'#(\\w+)', r'\\1', text_no_mentions)\n",
    "    print(\"After hashtag conversion:\", text_no_hashtags)\n",
    "    \n",
    "    # Test punctuation removal\n",
    "    text_no_punct = re.sub(r'[^\\w\\s]', '', text_no_hashtags)\n",
    "    print(\"After punctuation removal:\", text_no_punct)\n",
    "    \n",
    "    # Test whitespace cleaning\n",
    "    text_clean = re.sub(r'\\s+', ' ', text_no_punct).strip()\n",
    "    print(\"Final cleaned:\", text_clean)\n",
    "    \n",
    "    return text_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "h0QoPhx7zY4r"
   },
   "outputs": [],
   "source": [
    "def create_visualizations(df):\n",
    "    # 1. Sentiment Distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='sentiment', data=df)\n",
    "    plt.title('Distribution of Sentiments')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.savefig('fig/sentiment_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Text Length Distribution by Sentiment\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='sentiment', y='text_length', data=df)\n",
    "    plt.title('Text Length Distribution by Sentiment')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Text Length')\n",
    "    plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "    plt.savefig('fig/text_length_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Time-based Analysis\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Hourly distribution\n",
    "    sns.countplot(x='hour', hue='sentiment', data=df, ax=axes[0])\n",
    "    axes[0].set_title('Tweets by Hour of Day')\n",
    "    axes[0].set_xlabel('Hour')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    # Day of week distribution\n",
    "    sns.countplot(x='day_of_week', hue='sentiment', data=df, ax=axes[1])\n",
    "    axes[1].set_title('Tweets by Day of Week')\n",
    "    axes[1].set_xlabel('Day of Week (0=Monday)')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    # Monthly distribution\n",
    "    sns.countplot(x='month', hue='sentiment', data=df, ax=axes[2])\n",
    "    axes[2].set_title('Tweets by Month')\n",
    "    axes[2].set_xlabel('Month')\n",
    "    axes[2].set_ylabel('Count')\n",
    "    axes[2].legend(title='Sentiment', labels=['Negative', 'Positive'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/time_based_analysis.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Feature Correlation Analysis\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df[['sentiment', 'text_length', 'word_count', 'hashtag_count', 'mention_count', 'url_count']].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.savefig('fig/feature_correlation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. Word Clouds for Positive and Negative Tweets\n",
    "    def generate_wordcloud(text, title, filename):\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    \n",
    "    # Generate word clouds for positive and negative tweets\n",
    "    positive_text = ' '.join(df[df['sentiment'] == 1]['text'])\n",
    "    negative_text = ' '.join(df[df['sentiment'] == 0]['text'])\n",
    "    \n",
    "    generate_wordcloud(positive_text, 'Word Cloud for Positive Tweets', 'positive_wordcloud.png')\n",
    "    generate_wordcloud(negative_text, 'Word Cloud for Negative Tweets', 'negative_wordcloud.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Engineer features based on the project outline\n",
    "    \"\"\"\n",
    "    # 1. User-based Features\n",
    "    \n",
    "    # Group by user and calculate statistics\n",
    "    user_stats = df.groupby('user')['sentiment'].agg(['mean', 'count', 'std']).reset_index()\n",
    "    \n",
    "    # Calculate correct std with n-1 denominator\n",
    "    def adjusted_std(group):\n",
    "        if len(group) <= 1:\n",
    "            return 0\n",
    "        return np.std(group, ddof=1)  # ddof=1 uses n-1 denominator\n",
    "    \n",
    "    user_sentiment_std = df.groupby('user')['sentiment'].apply(adjusted_std)\n",
    "    user_stats['std'] = user_stats['user'].map(user_sentiment_std)\n",
    "    \n",
    "    # Handle case where a user has only one tweet (std is NaN)\n",
    "    user_stats['std'] = user_stats['std'].fillna(0)\n",
    "    \n",
    "    user_stats.columns = ['user', 'user_avg_sentiment', 'user_tweet_count', 'user_sentiment_std']\n",
    "    \n",
    "    # Merge user stats back to main dataframe\n",
    "    df = pd.merge(df, user_stats, on='user', how='left')\n",
    "    \n",
    "    # Calculate average posting gap time for each user\n",
    "    df = df.sort_values(['user', 'date'])\n",
    "    \n",
    "    # Function to calculate average time between posts\n",
    "    def calc_avg_gap(group):\n",
    "        if len(group) <= 1:\n",
    "            return pd.Timedelta(0)\n",
    "        gaps = group['date'].diff().dropna()\n",
    "        return gaps.mean()\n",
    "    \n",
    "    # Calculate average gap for each user\n",
    "    avg_gaps = df.groupby('user').apply(calc_avg_gap)\n",
    "    avg_gaps_seconds = avg_gaps.dt.total_seconds()\n",
    "    avg_gaps_df = pd.DataFrame({\n",
    "        'user': avg_gaps.index, \n",
    "        'avg_posting_gap_seconds': avg_gaps_seconds.values\n",
    "    })\n",
    "    \n",
    "    # Merge gaps back to main dataframe\n",
    "    df = pd.merge(df, avg_gaps_df, on='user', how='left')\n",
    "    df['avg_posting_gap_seconds'] = df['avg_posting_gap_seconds'].fillna(0)\n",
    "    \n",
    "    # 2. Text Processing Features\n",
    "    \n",
    "    # Apply text cleaning\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "    \n",
    "    # Create tokenized text for Word2Vec\n",
    "    df['tokens'] = df['clean_text'].apply(word_tokenize)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['tokens'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_word2vec_features(df, vector_size=100):\n",
    "    \"\"\"\n",
    "    Extract Word2Vec features\n",
    "    \"\"\"\n",
    "    # Train Word2Vec model\n",
    "    all_tokens = df['tokens'].tolist()\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=all_tokens,\n",
    "        vector_size=vector_size,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=4\n",
    "    )\n",
    "    \n",
    "    # Function to get document vectors by averaging word vectors\n",
    "    def get_doc_vector(tokens):\n",
    "        vec = np.zeros(vector_size)\n",
    "        count = 0\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                vec += w2v_model.wv[word]\n",
    "                count += 1\n",
    "            except KeyError:\n",
    "                # Word not in vocabulary\n",
    "                continue\n",
    "        if count > 0:\n",
    "            vec /= count\n",
    "        return vec\n",
    "    \n",
    "    # Get document vectors\n",
    "    doc_vectors = np.array(df['tokens'].apply(get_doc_vector).tolist())\n",
    "    w2v_df = pd.DataFrame(\n",
    "        doc_vectors,\n",
    "        columns=[f'w2v_{i}' for i in range(vector_size)]\n",
    "    )\n",
    "    \n",
    "    return w2v_df, w2v_model\n",
    "\n",
    "def select_features(features_df, n_components=50):\n",
    "    \"\"\"\n",
    "    Perform PCA for feature selection\n",
    "    \"\"\"\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform\n",
    "    pca_features = pca.fit_transform(features_df)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    pca_df = pd.DataFrame(\n",
    "        pca_features, \n",
    "        columns=[f'pca_{i}' for i in range(n_components)]\n",
    "    )\n",
    "    \n",
    "    # Calculate explained variance ratio\n",
    "    explained_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance)\n",
    "    \n",
    "    # Print variance explanation\n",
    "    print(f\"Top 10 components explain {cumulative_variance[9]:.2%} of variance\")\n",
    "    print(f\"All {n_components} components explain {cumulative_variance[-1]:.2%} of variance\")\n",
    "    \n",
    "    return pca_df, pca\n",
    "\n",
    "def train_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate classification models\n",
    "    \"\"\"\n",
    "    # Define base models\n",
    "    svm = SVC(probability=False, kernel='rbf')\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    xgb = GradientBoostingClassifier(n_estimators=100)\n",
    "    \n",
    "    # Train individual models\n",
    "    models = {\n",
    "        'SVM': svm,\n",
    "        'Random Forest': rf,\n",
    "        'XGBoost': xgb\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        t0 = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'accuracy': accuracy,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'classification_report': class_report\n",
    "        }\n",
    "        \n",
    "        print(f\"Training {name} took {time.time() - t0:.2f} seconds\")\n",
    "        print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "        print(f\"Classification Report:\\n{class_report}\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    # Create Voting Ensemble (majority voting)\n",
    "    voting_clf = VotingClassifier(\n",
    "        estimators=[('svm', svm), ('rf', rf), ('xgb', xgb)],\n",
    "        voting='hard'  # Majority voting\n",
    "    )\n",
    "    \n",
    "    print(\"Training Ensemble (Majority Voting)...\")\n",
    "    t0 = time.time()\n",
    "    voting_clf.fit(X_train, y_train)\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    class_report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    results['Ensemble'] = {\n",
    "        'model': voting_clf,\n",
    "        'accuracy': accuracy,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'classification_report': class_report\n",
    "    }\n",
    "    \n",
    "    print(\"Training Ensemble took %0.2f seconds\" % (time.time() - t0))\n",
    "    print(f\"Ensemble Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Classification Report:\\n{class_report}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def perform_cross_validation(X, y, df):\n",
    "    \"\"\"\n",
    "    Perform time-based and user-based cross-validation\n",
    "    \"\"\"\n",
    "    # Time-based Cross Validation\n",
    "    print(\"Performing Time-based Cross Validation\")\n",
    "    t0 = time.time()\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Use SVM as model for validation\n",
    "    model = SVC(kernel='rbf')\n",
    "    \n",
    "    time_scores = []\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        time_scores.append(score)\n",
    "    \n",
    "    print(f\"Time-based CV took {time.time() - t0:.2f} seconds\")\n",
    "    print(f\"Time-based CV Scores: {time_scores}\")\n",
    "    print(f\"Mean Time-based CV Score: {np.mean(time_scores):.4f}\")\n",
    "    \n",
    "    # User-based Cross Validation\n",
    "    print(\"\\nPerforming User-based Cross Validation\")\n",
    "    t0 = time.time()\n",
    "    user_groups = df['user'].astype('category').cat.codes.values\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    user_scores = []\n",
    "    for train_index, test_index in gkf.split(X, y, groups=user_groups):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        score = model.score(X_test, y_test)\n",
    "        user_scores.append(score)\n",
    "    \n",
    "    print(f\"User-based CV took {time.time() - t0:.2f} seconds\")\n",
    "    print(f\"User-based CV Scores: {user_scores}\")\n",
    "    print(f\"Mean User-based CV Score: {np.mean(user_scores):.4f}\")\n",
    "    \n",
    "    return time_scores, user_scores\n",
    "\n",
    "def analyze_misclassified_examples(df, X_test, y_test, model, idx_test):\n",
    "    \"\"\"\n",
    "    Analyze misclassified examples\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    misclassified_idx = idx_test[y_pred != y_test]\n",
    "    \n",
    "    misclassified_df = df.iloc[misclassified_idx].copy()\n",
    "    misclassified_df['predicted_sentiment'] = y_pred[y_pred != y_test]\n",
    "    \n",
    "    print(f\"Number of misclassified examples: {len(misclassified_df)}\")\n",
    "    \n",
    "    # Analyze by features\n",
    "    print(\"\\nMisclassification Analysis by Features:\")\n",
    "    \n",
    "    # By text length\n",
    "    print(\"\\nBy Text Length:\")\n",
    "    bins = [0, 50, 100, 150, 200, np.inf]\n",
    "    labels = ['Very Short', 'Short', 'Medium', 'Long', 'Very Long']\n",
    "    misclassified_df['text_length_bin'] = pd.cut(misclassified_df['text_length'], bins=bins, labels=labels)\n",
    "    print(misclassified_df['text_length_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # By user tweet count\n",
    "    print(\"\\nBy User Tweet Count:\")\n",
    "    bins = [0, 5, 10, 20, 50, np.inf]\n",
    "    labels = ['Very Few', 'Few', 'Average', 'Many', 'Very Many']\n",
    "    misclassified_df['user_tweet_count_bin'] = pd.cut(misclassified_df['user_tweet_count'], bins=bins, labels=labels)\n",
    "    print(misclassified_df['user_tweet_count_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # By time of day\n",
    "    print(\"\\nBy Hour of Day:\")\n",
    "    hour_bins = [0, 6, 12, 18, 24]\n",
    "    hour_labels = ['Night', 'Morning', 'Afternoon', 'Evening']\n",
    "    misclassified_df['hour_bin'] = pd.cut(misclassified_df['hour'], bins=hour_bins, labels=hour_labels)\n",
    "    print(misclassified_df['hour_bin'].value_counts(normalize=True).sort_index())\n",
    "    \n",
    "    # Sample of misclassified examples\n",
    "    print(\"\\nSample of Misclassified Examples:\")\n",
    "    sample = misclassified_df.sample(min(5, len(misclassified_df)))\n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"Text: {row['text']}\")\n",
    "        print(f\"True Sentiment: {row['sentiment']}, Predicted: {row['predicted_sentiment']}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return misclassified_df\n",
    "\n",
    "def visualize_results(df, results, pca, feature_names):\n",
    "    \"\"\"\n",
    "    Create visualizations for the analysis\n",
    "    \"\"\"\n",
    "    # 1. PCA Explained Variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('Explained Variance by Principal Component')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/pca_variance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Feature Importance from PCA loadings\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    # Get most important features from first component\n",
    "    component = 0\n",
    "    loadings = pd.Series(abs(pca.components_[component]), index=feature_names)\n",
    "    top_features = loadings.nlargest(15)\n",
    "    \n",
    "    sns.barplot(x=top_features.values, y=top_features.index)\n",
    "    plt.title(f'Top 15 Feature Importances (PC {component+1})')\n",
    "    plt.xlabel('Absolute Loading Value')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Sentiment Distribution by Time of Day\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    hour_counts = df.groupby(['hour', 'sentiment']).size().unstack()\n",
    "    hour_counts.plot(kind='bar', stacked=True)\n",
    "    plt.title('Sentiment Distribution by Hour of Day')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/sentiment_by_hour.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Sentiment Distribution by Day of Week\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    day_counts = df.groupby(['day_of_week', 'sentiment']).size().unstack()\n",
    "    day_counts.plot(kind='bar', stacked=True)\n",
    "    plt.title('Sentiment Distribution by Day of Week')\n",
    "    plt.xlabel('Day of Week (0=Monday)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/sentiment_by_day.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 5. User Sentiment Patterns (Top 10 users by tweet count)\n",
    "    top_users = df['user'].value_counts().head(10).index\n",
    "    user_df = df[df['user'].isin(top_users)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    user_sentiment = user_df.groupby('user')['sentiment'].mean().sort_values()\n",
    "    sns.barplot(x=user_sentiment.index, y=user_sentiment.values)\n",
    "    plt.title('Average Sentiment for Top 10 Users')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/user_sentiment.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 6. Word Clouds by Sentiment\n",
    "    for sentiment, label in [(0, 'Negative'), (1, 'Positive')]:\n",
    "        text = ' '.join(df[df['sentiment'] == sentiment]['clean_text'])\n",
    "        \n",
    "        wordcloud = WordCloud(\n",
    "            width=800, height=400,\n",
    "            background_color='white',\n",
    "            max_words=200\n",
    "        ).generate(text)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Word Cloud for {label} Sentiment')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fig/wordcloud_sentiment_{sentiment}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # 7. Model Comparison\n",
    "    accuracies = {name: info['accuracy'] for name, info in results.items()}\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()))\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('fig/model_comparison.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 8. Confusion Matrix Visualization\n",
    "    for name, info in results.items():\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = info['confusion_matrix']\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                   xticklabels=['Negative', 'Positive'],\n",
    "                   yticklabels=['Negative', 'Positive'])\n",
    "        plt.title(f'Confusion Matrix - {name}')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'fig/confusion_matrix_{name}.png')\n",
    "        plt.close()\n",
    "\n",
    "def compare_processing_methods(df, test_size=1000):\n",
    "    \"\"\"\n",
    "    Compare local vs distributed processing performance\n",
    "    \"\"\"\n",
    "    # Subset data for testing\n",
    "    test_df = df.sample(test_size, random_state=42)\n",
    "    \n",
    "    # 1. Local Python Implementation\n",
    "    print(\"Testing Local Python Implementation...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Simulate local processing\n",
    "    tokens = test_df['text'].apply(clean_text).apply(word_tokenize).tolist()\n",
    "    local_time = time.time() - start_time\n",
    "    print(f\"Local processing time: {local_time:.2f} seconds\")\n",
    "    \n",
    "    # 2. Simulated Distributed Processing\n",
    "    try:\n",
    "        print(\"\\nTesting Parallel Processing Implementation...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Determine number of cores\n",
    "        num_cores = multiprocessing.cpu_count()\n",
    "        print(f\"Using {num_cores} cores\")\n",
    "        \n",
    "        # Split data into chunks\n",
    "        chunks = np.array_split(test_df['text'], num_cores)\n",
    "        \n",
    "        # Define processing function\n",
    "        def process_chunk(chunk):\n",
    "            return [word_tokenize(clean_text(text)) for text in chunk]\n",
    "        \n",
    "        # Create a pool and process in parallel\n",
    "        with multiprocessing.Pool(num_cores) as pool:\n",
    "            results = pool.map(process_chunk, chunks)\n",
    "            \n",
    "        # Flatten results\n",
    "        parallel_tokens = [item for sublist in results for item in sublist]\n",
    "        \n",
    "        parallel_time = time.time() - start_time\n",
    "        print(f\"Parallel processing time: {parallel_time:.2f} seconds\")\n",
    "        print(f\"Speedup: {local_time / parallel_time:.2f}x\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in parallel processing: {e}\")\n",
    "        print(\"Please set up a proper distributed environment for actual testing\")\n",
    "        parallel_time = None\n",
    "    \n",
    "    return {'local_time': local_time, 'parallel_time': parallel_time}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing data...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>2009-04-06 22:19:45</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>115</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>2009-04-06 22:19:49</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>2009-04-06 22:19:53</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>89</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>2009-04-06 22:19:57</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>111</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                date     query             user  \\\n",
       "0          0  1467810369 2009-04-06 22:19:45  NO_QUERY  _TheSpecialOne_   \n",
       "1          0  1467810672 2009-04-06 22:19:49  NO_QUERY    scotthamilton   \n",
       "2          0  1467810917 2009-04-06 22:19:53  NO_QUERY         mattycus   \n",
       "3          0  1467811184 2009-04-06 22:19:57  NO_QUERY          ElleCTF   \n",
       "4          0  1467811193 2009-04-06 22:19:57  NO_QUERY           Karoli   \n",
       "\n",
       "                                                text  text_length  word_count  \\\n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...          115          19   \n",
       "1  is upset that he can't update his Facebook by ...          111          21   \n",
       "2  @Kenichan I dived many times for the ball. Man...           89          18   \n",
       "3    my whole body feels itchy and like its on fire            47          10   \n",
       "4  @nationwideclass no, it's not behaving at all....          111          21   \n",
       "\n",
       "   hashtag_count  mention_count  url_count  hour  day_of_week  month  \n",
       "0              0              1          1    22            0      4  \n",
       "1              0              0          0    22            0      4  \n",
       "2              0              1          0    22            0      4  \n",
       "3              0              0          0    22            0      4  \n",
       "4              0              1          0    22            0      4  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading and preprocessing data...\")\n",
    "df_all = load_and_preprocess_data('sentiment140.csv')\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.sample(10000, replace=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basic statistics:\n",
      "Dataset shape: (10000, 14)\n",
      "Sentiment distribution: sentiment\n",
      "0    0.5004\n",
      "1    0.4996\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBasic statistics:\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Sentiment distribution: {df['sentiment'].value_counts(normalize=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Engineering features...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>user_avg_sentiment</th>\n",
       "      <th>user_tweet_count</th>\n",
       "      <th>user_sentiment_std</th>\n",
       "      <th>avg_posting_gap_seconds</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1573020777</td>\n",
       "      <td>2009-04-20 23:24:38</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>00kate00</td>\n",
       "      <td>thats was the fastest shower of my life, someb...</td>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats was the fastest shower of my life somebo...</td>\n",
       "      <td>[thats, fastest, shower, life, somebody, kept,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2016177197</td>\n",
       "      <td>2009-06-03 06:18:47</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>01johnn</td>\n",
       "      <td>in engli8sh... doing CPT again &amp;gt;_&amp;gt; last ...</td>\n",
       "      <td>73</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>in engli8sh doing cpt again gt_gt last day cel...</td>\n",
       "      <td>[engli8sh, cpt, gt_gt, last, day, cell, hours]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2047773429</td>\n",
       "      <td>2009-06-05 14:09:32</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>10butterflys</td>\n",
       "      <td>@heavenunaware ouch! did they give anything 4 ...</td>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ouch did they give anything 4 painwhy couldnt ...</td>\n",
       "      <td>[ouch, give, anything, 4, painwhy, couldnt, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1695537093</td>\n",
       "      <td>2009-05-04 06:22:40</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>12gaugecows</td>\n",
       "      <td>ahh im gonna go to bed and sleep wats left of ...</td>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ahh im gonna go to bed and sleep wats left of ...</td>\n",
       "      <td>[ahh, im, gon, na, go, bed, sleep, wats, left,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2177133119</td>\n",
       "      <td>2009-06-15 05:31:02</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>12thCenturyFox</td>\n",
       "      <td>Tehehe, so many Supernatural fans rather peeve...</td>\n",
       "      <td>123</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tehehe so many supernatural fans rather peeved...</td>\n",
       "      <td>[tehehe, many, supernatural, fans, rather, pee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                date     query            user  \\\n",
       "0          0  1573020777 2009-04-20 23:24:38  NO_QUERY        00kate00   \n",
       "1          1  2016177197 2009-06-03 06:18:47  NO_QUERY         01johnn   \n",
       "2          0  2047773429 2009-06-05 14:09:32  NO_QUERY    10butterflys   \n",
       "3          0  1695537093 2009-05-04 06:22:40  NO_QUERY     12gaugecows   \n",
       "4          1  2177133119 2009-06-15 05:31:02  NO_QUERY  12thCenturyFox   \n",
       "\n",
       "                                                text  text_length  word_count  \\\n",
       "0  thats was the fastest shower of my life, someb...           98          19   \n",
       "1  in engli8sh... doing CPT again &gt;_&gt; last ...           73          14   \n",
       "2  @heavenunaware ouch! did they give anything 4 ...          111          18   \n",
       "3  ahh im gonna go to bed and sleep wats left of ...           65          15   \n",
       "4  Tehehe, so many Supernatural fans rather peeve...          123          21   \n",
       "\n",
       "   hashtag_count  mention_count  url_count  hour  day_of_week  month  \\\n",
       "0              0              0          0    23            0      4   \n",
       "1              0              0          0     6            2      6   \n",
       "2              0              1          0    14            4      6   \n",
       "3              0              0          0     6            0      5   \n",
       "4              0              0          0     5            0      6   \n",
       "\n",
       "   user_avg_sentiment  user_tweet_count  user_sentiment_std  \\\n",
       "0                 0.0                 1                 0.0   \n",
       "1                 1.0                 1                 0.0   \n",
       "2                 0.0                 1                 0.0   \n",
       "3                 0.0                 1                 0.0   \n",
       "4                 1.0                 1                 0.0   \n",
       "\n",
       "   avg_posting_gap_seconds                                         clean_text  \\\n",
       "0                      0.0  thats was the fastest shower of my life somebo...   \n",
       "1                      0.0  in engli8sh doing cpt again gt_gt last day cel...   \n",
       "2                      0.0  ouch did they give anything 4 painwhy couldnt ...   \n",
       "3                      0.0  ahh im gonna go to bed and sleep wats left of ...   \n",
       "4                      0.0  tehehe so many supernatural fans rather peeved...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [thats, fastest, shower, life, somebody, kept,...  \n",
       "1     [engli8sh, cpt, gt_gt, last, day, cell, hours]  \n",
       "2  [ouch, give, anything, 4, painwhy, couldnt, to...  \n",
       "3  [ahh, im, gon, na, go, bed, sleep, wats, left,...  \n",
       "4  [tehehe, many, supernatural, fans, rather, pee...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEngineering features...\")\n",
    "df = engineer_features(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Word2Vec features...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nExtracting Word2Vec features...\")\n",
    "w2v_df, w2v_model = extract_word2vec_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numeric features with text features, remove highly correlated features (user_avg_sentiment)\n",
    "numeric_features = df[['text_length', 'word_count', 'hashtag_count', \n",
    "                        'mention_count', 'url_count', 'hour', 'day_of_week', \n",
    "                        'month', 'user_tweet_count', \n",
    "                        'user_sentiment_std', 'avg_posting_gap_seconds']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of highly correlated feature pairs: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>user_tweet_count</th>\n",
       "      <th>user_sentiment_std</th>\n",
       "      <th>avg_posting_gap_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954755</td>\n",
       "      <td>0.084414</td>\n",
       "      <td>0.169021</td>\n",
       "      <td>0.085556</td>\n",
       "      <td>-0.005803</td>\n",
       "      <td>-0.010663</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.027241</td>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.022404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_count</th>\n",
       "      <td>0.954755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>0.088909</td>\n",
       "      <td>-0.024535</td>\n",
       "      <td>-0.002258</td>\n",
       "      <td>-0.005395</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>0.012672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hashtag_count</th>\n",
       "      <td>0.084414</td>\n",
       "      <td>0.050085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>-0.030075</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>-0.004383</td>\n",
       "      <td>0.018234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mention_count</th>\n",
       "      <td>0.169021</td>\n",
       "      <td>0.088909</td>\n",
       "      <td>0.006434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056854</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>-0.018483</td>\n",
       "      <td>-0.025903</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>0.047612</td>\n",
       "      <td>0.048601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>url_count</th>\n",
       "      <td>0.085556</td>\n",
       "      <td>-0.024535</td>\n",
       "      <td>-0.002095</td>\n",
       "      <td>-0.056854</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021421</td>\n",
       "      <td>-0.018023</td>\n",
       "      <td>-0.013231</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.028787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hour</th>\n",
       "      <td>-0.005803</td>\n",
       "      <td>-0.002258</td>\n",
       "      <td>-0.030075</td>\n",
       "      <td>0.010705</td>\n",
       "      <td>-0.021421</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.050252</td>\n",
       "      <td>0.096377</td>\n",
       "      <td>-0.050718</td>\n",
       "      <td>-0.025361</td>\n",
       "      <td>-0.050066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day_of_week</th>\n",
       "      <td>-0.010663</td>\n",
       "      <td>-0.005395</td>\n",
       "      <td>0.007988</td>\n",
       "      <td>-0.018483</td>\n",
       "      <td>-0.018023</td>\n",
       "      <td>0.050252</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.260821</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>0.019667</td>\n",
       "      <td>0.013942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>-0.025903</td>\n",
       "      <td>-0.013231</td>\n",
       "      <td>0.096377</td>\n",
       "      <td>-0.260821</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009186</td>\n",
       "      <td>-0.015644</td>\n",
       "      <td>-0.052630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_tweet_count</th>\n",
       "      <td>0.027241</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.007361</td>\n",
       "      <td>0.093536</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.050718</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>-0.009186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>0.487830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_sentiment_std</th>\n",
       "      <td>0.008381</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>-0.004383</td>\n",
       "      <td>0.047612</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>-0.025361</td>\n",
       "      <td>0.019667</td>\n",
       "      <td>-0.015644</td>\n",
       "      <td>0.473877</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.452354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_posting_gap_seconds</th>\n",
       "      <td>0.022404</td>\n",
       "      <td>0.012672</td>\n",
       "      <td>0.018234</td>\n",
       "      <td>0.048601</td>\n",
       "      <td>0.028787</td>\n",
       "      <td>-0.050066</td>\n",
       "      <td>0.013942</td>\n",
       "      <td>-0.052630</td>\n",
       "      <td>0.487830</td>\n",
       "      <td>0.452354</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         text_length  word_count  hashtag_count  \\\n",
       "text_length                 1.000000    0.954755       0.084414   \n",
       "word_count                  0.954755    1.000000       0.050085   \n",
       "hashtag_count               0.084414    0.050085       1.000000   \n",
       "mention_count               0.169021    0.088909       0.006434   \n",
       "url_count                   0.085556   -0.024535      -0.002095   \n",
       "hour                       -0.005803   -0.002258      -0.030075   \n",
       "day_of_week                -0.010663   -0.005395       0.007988   \n",
       "month                       0.001640    0.011387       0.003241   \n",
       "user_tweet_count            0.027241    0.017461       0.007361   \n",
       "user_sentiment_std          0.008381    0.004821      -0.004383   \n",
       "avg_posting_gap_seconds     0.022404    0.012672       0.018234   \n",
       "\n",
       "                         mention_count  url_count      hour  day_of_week  \\\n",
       "text_length                   0.169021   0.085556 -0.005803    -0.010663   \n",
       "word_count                    0.088909  -0.024535 -0.002258    -0.005395   \n",
       "hashtag_count                 0.006434  -0.002095 -0.030075     0.007988   \n",
       "mention_count                 1.000000  -0.056854  0.010705    -0.018483   \n",
       "url_count                    -0.056854   1.000000 -0.021421    -0.018023   \n",
       "hour                          0.010705  -0.021421  1.000000     0.050252   \n",
       "day_of_week                  -0.018483  -0.018023  0.050252     1.000000   \n",
       "month                        -0.025903  -0.013231  0.096377    -0.260821   \n",
       "user_tweet_count              0.093536  -0.000214 -0.050718     0.005455   \n",
       "user_sentiment_std            0.047612   0.009790 -0.025361     0.019667   \n",
       "avg_posting_gap_seconds       0.048601   0.028787 -0.050066     0.013942   \n",
       "\n",
       "                            month  user_tweet_count  user_sentiment_std  \\\n",
       "text_length              0.001640          0.027241            0.008381   \n",
       "word_count               0.011387          0.017461            0.004821   \n",
       "hashtag_count            0.003241          0.007361           -0.004383   \n",
       "mention_count           -0.025903          0.093536            0.047612   \n",
       "url_count               -0.013231         -0.000214            0.009790   \n",
       "hour                     0.096377         -0.050718           -0.025361   \n",
       "day_of_week             -0.260821          0.005455            0.019667   \n",
       "month                    1.000000         -0.009186           -0.015644   \n",
       "user_tweet_count        -0.009186          1.000000            0.473877   \n",
       "user_sentiment_std      -0.015644          0.473877            1.000000   \n",
       "avg_posting_gap_seconds -0.052630          0.487830            0.452354   \n",
       "\n",
       "                         avg_posting_gap_seconds  \n",
       "text_length                             0.022404  \n",
       "word_count                              0.012672  \n",
       "hashtag_count                           0.018234  \n",
       "mention_count                           0.048601  \n",
       "url_count                               0.028787  \n",
       "hour                                   -0.050066  \n",
       "day_of_week                             0.013942  \n",
       "month                                  -0.052630  \n",
       "user_tweet_count                        0.487830  \n",
       "user_sentiment_std                      0.452354  \n",
       "avg_posting_gap_seconds                 1.000000  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_matrix = numeric_features.corr()\n",
    "high_correlations = (correlation_matrix.abs() > 0.8) & (correlation_matrix.abs() < 1.0)\n",
    "print(f\"Number of highly correlated feature pairs: {high_correlations.sum().sum() // 2}\")\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>hashtag_count</th>\n",
       "      <th>mention_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>user_tweet_count</th>\n",
       "      <th>user_sentiment_std</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v_90</th>\n",
       "      <th>w2v_91</th>\n",
       "      <th>w2v_92</th>\n",
       "      <th>w2v_93</th>\n",
       "      <th>w2v_94</th>\n",
       "      <th>w2v_95</th>\n",
       "      <th>w2v_96</th>\n",
       "      <th>w2v_97</th>\n",
       "      <th>w2v_98</th>\n",
       "      <th>w2v_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256080</td>\n",
       "      <td>0.095325</td>\n",
       "      <td>-0.005306</td>\n",
       "      <td>0.022826</td>\n",
       "      <td>0.292922</td>\n",
       "      <td>0.182167</td>\n",
       "      <td>0.124928</td>\n",
       "      <td>-0.194928</td>\n",
       "      <td>0.052775</td>\n",
       "      <td>-0.011758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276297</td>\n",
       "      <td>0.099100</td>\n",
       "      <td>-0.001931</td>\n",
       "      <td>0.026111</td>\n",
       "      <td>0.316984</td>\n",
       "      <td>0.196012</td>\n",
       "      <td>0.133682</td>\n",
       "      <td>-0.203115</td>\n",
       "      <td>0.055438</td>\n",
       "      <td>-0.015974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154568</td>\n",
       "      <td>0.053209</td>\n",
       "      <td>-0.007994</td>\n",
       "      <td>0.010790</td>\n",
       "      <td>0.172207</td>\n",
       "      <td>0.110940</td>\n",
       "      <td>0.074244</td>\n",
       "      <td>-0.115177</td>\n",
       "      <td>0.033252</td>\n",
       "      <td>-0.007060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417099</td>\n",
       "      <td>0.146936</td>\n",
       "      <td>-0.006537</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>0.469339</td>\n",
       "      <td>0.289272</td>\n",
       "      <td>0.208701</td>\n",
       "      <td>-0.314249</td>\n",
       "      <td>0.088994</td>\n",
       "      <td>-0.031013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>123</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101580</td>\n",
       "      <td>0.038096</td>\n",
       "      <td>-0.003880</td>\n",
       "      <td>0.007097</td>\n",
       "      <td>0.112110</td>\n",
       "      <td>0.071939</td>\n",
       "      <td>0.049285</td>\n",
       "      <td>-0.079152</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>-0.001952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>86</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.313657</td>\n",
       "      <td>0.114965</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>0.027011</td>\n",
       "      <td>0.350745</td>\n",
       "      <td>0.223468</td>\n",
       "      <td>0.150851</td>\n",
       "      <td>-0.231760</td>\n",
       "      <td>0.064528</td>\n",
       "      <td>-0.016540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>112</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.283202</td>\n",
       "      <td>0.101469</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>0.023188</td>\n",
       "      <td>0.314071</td>\n",
       "      <td>0.202886</td>\n",
       "      <td>0.137725</td>\n",
       "      <td>-0.208752</td>\n",
       "      <td>0.056263</td>\n",
       "      <td>-0.013447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280665</td>\n",
       "      <td>0.097638</td>\n",
       "      <td>-0.005766</td>\n",
       "      <td>0.028798</td>\n",
       "      <td>0.318183</td>\n",
       "      <td>0.195324</td>\n",
       "      <td>0.136576</td>\n",
       "      <td>-0.206319</td>\n",
       "      <td>0.061062</td>\n",
       "      <td>-0.012510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146276</td>\n",
       "      <td>0.057975</td>\n",
       "      <td>-0.008761</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.164181</td>\n",
       "      <td>0.102883</td>\n",
       "      <td>0.070963</td>\n",
       "      <td>-0.110034</td>\n",
       "      <td>0.031665</td>\n",
       "      <td>-0.004307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>62</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071375</td>\n",
       "      <td>0.026825</td>\n",
       "      <td>-0.000181</td>\n",
       "      <td>0.008196</td>\n",
       "      <td>0.078792</td>\n",
       "      <td>0.049918</td>\n",
       "      <td>0.033633</td>\n",
       "      <td>-0.056523</td>\n",
       "      <td>0.014522</td>\n",
       "      <td>-0.006370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows  111 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_length  word_count  hashtag_count  mention_count  url_count  hour  \\\n",
       "0              98          19              0              0          0    23   \n",
       "1              73          14              0              0          0     6   \n",
       "2             111          18              0              1          0    14   \n",
       "3              65          15              0              0          0     6   \n",
       "4             123          21              0              0          0     5   \n",
       "...           ...         ...            ...            ...        ...   ...   \n",
       "9995           86          14              0              0          0     9   \n",
       "9996          112          20              0              1          0    22   \n",
       "9997           41           7              0              1          0    17   \n",
       "9998           58          10              0              1          0     0   \n",
       "9999           62          12              0              0          0     7   \n",
       "\n",
       "      day_of_week  month  user_tweet_count  user_sentiment_std  ...    w2v_90  \\\n",
       "0               0      4                 1                 0.0  ...  0.256080   \n",
       "1               2      6                 1                 0.0  ...  0.276297   \n",
       "2               4      6                 1                 0.0  ...  0.154568   \n",
       "3               0      5                 1                 0.0  ...  0.417099   \n",
       "4               0      6                 1                 0.0  ...  0.101580   \n",
       "...           ...    ...               ...                 ...  ...       ...   \n",
       "9995            5      4                 1                 0.0  ...  0.313657   \n",
       "9996            3      5                 1                 0.0  ...  0.283202   \n",
       "9997            4      6                 1                 0.0  ...  0.280665   \n",
       "9998            4      5                 1                 0.0  ...  0.146276   \n",
       "9999            6      4                 1                 0.0  ...  0.071375   \n",
       "\n",
       "        w2v_91    w2v_92    w2v_93    w2v_94    w2v_95    w2v_96    w2v_97  \\\n",
       "0     0.095325 -0.005306  0.022826  0.292922  0.182167  0.124928 -0.194928   \n",
       "1     0.099100 -0.001931  0.026111  0.316984  0.196012  0.133682 -0.203115   \n",
       "2     0.053209 -0.007994  0.010790  0.172207  0.110940  0.074244 -0.115177   \n",
       "3     0.146936 -0.006537  0.033354  0.469339  0.289272  0.208701 -0.314249   \n",
       "4     0.038096 -0.003880  0.007097  0.112110  0.071939  0.049285 -0.079152   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  0.114965 -0.006700  0.027011  0.350745  0.223468  0.150851 -0.231760   \n",
       "9996  0.101469 -0.002345  0.023188  0.314071  0.202886  0.137725 -0.208752   \n",
       "9997  0.097638 -0.005766  0.028798  0.318183  0.195324  0.136576 -0.206319   \n",
       "9998  0.057975 -0.008761  0.009294  0.164181  0.102883  0.070963 -0.110034   \n",
       "9999  0.026825 -0.000181  0.008196  0.078792  0.049918  0.033633 -0.056523   \n",
       "\n",
       "        w2v_98    w2v_99  \n",
       "0     0.052775 -0.011758  \n",
       "1     0.055438 -0.015974  \n",
       "2     0.033252 -0.007060  \n",
       "3     0.088994 -0.031013  \n",
       "4     0.023295 -0.001952  \n",
       "...        ...       ...  \n",
       "9995  0.064528 -0.016540  \n",
       "9996  0.056263 -0.013447  \n",
       "9997  0.061062 -0.012510  \n",
       "9998  0.031665 -0.004307  \n",
       "9999  0.014522 -0.006370  \n",
       "\n",
       "[10000 rows x 111 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Reset index to ensure proper concatenation\n",
    "numeric_features = numeric_features.reset_index(drop=True)\n",
    "w2v_df = w2v_df.reset_index(drop=True)\n",
    "\n",
    "# Combine all features\n",
    "all_features = pd.concat([numeric_features, w2v_df], axis=1)\n",
    "all_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing feature selection with PCA...\n",
      "Top 10 components explain 100.00% of variance\n",
      "All 50 components explain 100.00% of variance\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming feature selection with PCA...\")\n",
    "feature_names = all_features.columns\n",
    "pca_df, pca = select_features(all_features)\n",
    "\n",
    "# Define target variable\n",
    "y = df['sentiment']\n",
    "\n",
    "# Split data (keeping track of original indices)\n",
    "df_index = df.index\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    pca_df, y, df_index, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating models...\n",
      "Training SVM...\n",
      "Training SVM took 5.61 seconds\n",
      "SVM Accuracy: 0.5020\n",
      "Confusion Matrix:\n",
      "[[970  31]\n",
      " [965  34]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.97      0.66      1001\n",
      "           1       0.52      0.03      0.06       999\n",
      "\n",
      "    accuracy                           0.50      2000\n",
      "   macro avg       0.51      0.50      0.36      2000\n",
      "weighted avg       0.51      0.50      0.36      2000\n",
      "\n",
      "==================================================\n",
      "Training Random Forest...\n",
      "Training Random Forest took 6.34 seconds\n",
      "Random Forest Accuracy: 0.6780\n",
      "Confusion Matrix:\n",
      "[[676 325]\n",
      " [319 680]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.68      0.68      1001\n",
      "           1       0.68      0.68      0.68       999\n",
      "\n",
      "    accuracy                           0.68      2000\n",
      "   macro avg       0.68      0.68      0.68      2000\n",
      "weighted avg       0.68      0.68      0.68      2000\n",
      "\n",
      "==================================================\n",
      "Training XGBoost...\n",
      "Training XGBoost took 18.09 seconds\n",
      "XGBoost Accuracy: 0.6730\n",
      "Confusion Matrix:\n",
      "[[670 331]\n",
      " [323 676]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.67      0.67      1001\n",
      "           1       0.67      0.68      0.67       999\n",
      "\n",
      "    accuracy                           0.67      2000\n",
      "   macro avg       0.67      0.67      0.67      2000\n",
      "weighted avg       0.67      0.67      0.67      2000\n",
      "\n",
      "==================================================\n",
      "Training Ensemble (Majority Voting)...\n",
      "Training Ensemble took 29.80 seconds\n",
      "Ensemble Accuracy: 0.6765\n",
      "Confusion Matrix:\n",
      "[[742 259]\n",
      " [388 611]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.70      1001\n",
      "           1       0.70      0.61      0.65       999\n",
      "\n",
      "    accuracy                           0.68      2000\n",
      "   macro avg       0.68      0.68      0.68      2000\n",
      "weighted avg       0.68      0.68      0.68      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining and evaluating models...\")\n",
    "results = train_evaluate_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "best_svm = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing cross-validation...\n",
      "Performing Time-based Cross Validation\n",
      "Time-based CV took 14.15 seconds\n",
      "Time-based CV Scores: [0.514405762304922, 0.503001200480192, 0.4891956782713085, 0.5132052821128451, 0.5054021608643458]\n",
      "Mean Time-based CV Score: 0.5050\n",
      "\n",
      "Performing User-based Cross Validation\n",
      "User-based CV took 28.79 seconds\n",
      "User-based CV Scores: [0.516, 0.485, 0.511, 0.502, 0.508]\n",
      "Mean User-based CV Score: 0.5044\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming cross-validation...\")\n",
    "time_scores, user_scores = perform_cross_validation(pca_df, y, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing misclassified examples...\n",
      "Number of misclassified examples: 644\n",
      "\n",
      "Misclassification Analysis by Features:\n",
      "\n",
      "By Text Length:\n",
      "text_length_bin\n",
      "Very Short    0.324534\n",
      "Short         0.372671\n",
      "Medium        0.302795\n",
      "Long          0.000000\n",
      "Very Long     0.000000\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "By User Tweet Count:\n",
      "user_tweet_count_bin\n",
      "Very Few     1.0\n",
      "Few          0.0\n",
      "Average      0.0\n",
      "Many         0.0\n",
      "Very Many    0.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "By Hour of Day:\n",
      "hour_bin\n",
      "Night        0.296173\n",
      "Morning      0.286190\n",
      "Afternoon    0.214642\n",
      "Evening      0.202995\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Sample of Misclassified Examples:\n",
      "Text: thinks today will be long, but rewarding  2 more days until A.J.'s wedding!\n",
      "True Sentiment: 1, Predicted: 0\n",
      "--------------------------------------------------\n",
      "Text: just got out....not as bad as I was expecting \n",
      "True Sentiment: 1, Predicted: 0\n",
      "--------------------------------------------------\n",
      "Text: is still waiting for my number 2 transfer onto my new sim card \n",
      "True Sentiment: 1, Predicted: 0\n",
      "--------------------------------------------------\n",
      "Text: Watched Crank 2.... awesome \n",
      "True Sentiment: 1, Predicted: 0\n",
      "--------------------------------------------------\n",
      "Text: @madnilk a, gotcha! well, i have no idea how many malaysians are on our list - but don't think there are many \n",
      "True Sentiment: 1, Predicted: 0\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAnalyzing misclassified examples...\")\n",
    "best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "best_model = results[best_model_name]['model']\n",
    "misclassified_df = analyze_misclassified_examples(df, X_test, y_test, best_model, idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating visualizations...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nCreating visualizations...\")\n",
    "visualize_results(df, results, pca, feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing processing methods...\n",
      "Testing Local Python Implementation...\n",
      "Local processing time: 0.08 seconds\n",
      "\n",
      "Testing Parallel Processing Implementation...\n",
      "Using 8 cores\n",
      "Error in parallel processing: Can't pickle local object 'compare_processing_methods.<locals>.process_chunk'\n",
      "Please set up a proper distributed environment for actual testing\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nComparing processing methods...\")\n",
    "performance_results = compare_processing_methods(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "sentiment_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
